{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Genre Classifier\n",
    "\n",
    "### Description\n",
    "[Description of project goes here]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Import Libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>length</th>\n",
       "      <th>chroma_stft_mean</th>\n",
       "      <th>chroma_stft_var</th>\n",
       "      <th>rms_mean</th>\n",
       "      <th>rms_var</th>\n",
       "      <th>spectral_centroid_mean</th>\n",
       "      <th>spectral_centroid_var</th>\n",
       "      <th>spectral_bandwidth_mean</th>\n",
       "      <th>spectral_bandwidth_var</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc16_var</th>\n",
       "      <th>mfcc17_mean</th>\n",
       "      <th>mfcc17_var</th>\n",
       "      <th>mfcc18_mean</th>\n",
       "      <th>mfcc18_var</th>\n",
       "      <th>mfcc19_mean</th>\n",
       "      <th>mfcc19_var</th>\n",
       "      <th>mfcc20_mean</th>\n",
       "      <th>mfcc20_var</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blues.00000.0.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.335406</td>\n",
       "      <td>0.091048</td>\n",
       "      <td>0.130405</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>1773.065032</td>\n",
       "      <td>167541.630869</td>\n",
       "      <td>1972.744388</td>\n",
       "      <td>117335.771563</td>\n",
       "      <td>...</td>\n",
       "      <td>39.687145</td>\n",
       "      <td>-3.241280</td>\n",
       "      <td>36.488243</td>\n",
       "      <td>0.722209</td>\n",
       "      <td>38.099152</td>\n",
       "      <td>-5.050335</td>\n",
       "      <td>33.618073</td>\n",
       "      <td>-0.243027</td>\n",
       "      <td>43.771767</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blues.00000.1.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.343065</td>\n",
       "      <td>0.086147</td>\n",
       "      <td>0.112699</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>1816.693777</td>\n",
       "      <td>90525.690866</td>\n",
       "      <td>2010.051501</td>\n",
       "      <td>65671.875673</td>\n",
       "      <td>...</td>\n",
       "      <td>64.748276</td>\n",
       "      <td>-6.055294</td>\n",
       "      <td>40.677654</td>\n",
       "      <td>0.159015</td>\n",
       "      <td>51.264091</td>\n",
       "      <td>-2.837699</td>\n",
       "      <td>97.030830</td>\n",
       "      <td>5.784063</td>\n",
       "      <td>59.943081</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blues.00000.2.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.346815</td>\n",
       "      <td>0.092243</td>\n",
       "      <td>0.132003</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>1788.539719</td>\n",
       "      <td>111407.437613</td>\n",
       "      <td>2084.565132</td>\n",
       "      <td>75124.921716</td>\n",
       "      <td>...</td>\n",
       "      <td>67.336563</td>\n",
       "      <td>-1.768610</td>\n",
       "      <td>28.348579</td>\n",
       "      <td>2.378768</td>\n",
       "      <td>45.717648</td>\n",
       "      <td>-1.938424</td>\n",
       "      <td>53.050835</td>\n",
       "      <td>2.517375</td>\n",
       "      <td>33.105122</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blues.00000.3.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.363639</td>\n",
       "      <td>0.086856</td>\n",
       "      <td>0.132565</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>1655.289045</td>\n",
       "      <td>111952.284517</td>\n",
       "      <td>1960.039988</td>\n",
       "      <td>82913.639269</td>\n",
       "      <td>...</td>\n",
       "      <td>47.739452</td>\n",
       "      <td>-3.841155</td>\n",
       "      <td>28.337118</td>\n",
       "      <td>1.218588</td>\n",
       "      <td>34.770935</td>\n",
       "      <td>-3.580352</td>\n",
       "      <td>50.836224</td>\n",
       "      <td>3.630866</td>\n",
       "      <td>32.023678</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blues.00000.4.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.335579</td>\n",
       "      <td>0.088129</td>\n",
       "      <td>0.143289</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>1630.656199</td>\n",
       "      <td>79667.267654</td>\n",
       "      <td>1948.503884</td>\n",
       "      <td>60204.020268</td>\n",
       "      <td>...</td>\n",
       "      <td>30.336359</td>\n",
       "      <td>0.664582</td>\n",
       "      <td>45.880913</td>\n",
       "      <td>1.689446</td>\n",
       "      <td>51.363583</td>\n",
       "      <td>-3.392489</td>\n",
       "      <td>26.738789</td>\n",
       "      <td>0.536961</td>\n",
       "      <td>29.146694</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>blues.00000.5.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.376670</td>\n",
       "      <td>0.089702</td>\n",
       "      <td>0.132618</td>\n",
       "      <td>0.003583</td>\n",
       "      <td>1994.915219</td>\n",
       "      <td>211700.619569</td>\n",
       "      <td>2152.767854</td>\n",
       "      <td>74263.873102</td>\n",
       "      <td>...</td>\n",
       "      <td>31.448069</td>\n",
       "      <td>-3.448373</td>\n",
       "      <td>34.284130</td>\n",
       "      <td>-0.416165</td>\n",
       "      <td>40.791092</td>\n",
       "      <td>-3.649625</td>\n",
       "      <td>32.457901</td>\n",
       "      <td>3.025218</td>\n",
       "      <td>28.892687</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>blues.00000.6.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.379909</td>\n",
       "      <td>0.088827</td>\n",
       "      <td>0.130335</td>\n",
       "      <td>0.003166</td>\n",
       "      <td>1962.150096</td>\n",
       "      <td>177443.070045</td>\n",
       "      <td>2146.503479</td>\n",
       "      <td>98020.541422</td>\n",
       "      <td>...</td>\n",
       "      <td>33.954071</td>\n",
       "      <td>-2.068194</td>\n",
       "      <td>25.623655</td>\n",
       "      <td>1.428141</td>\n",
       "      <td>47.957699</td>\n",
       "      <td>-3.267124</td>\n",
       "      <td>39.382240</td>\n",
       "      <td>3.276939</td>\n",
       "      <td>25.999132</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>blues.00000.7.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.331880</td>\n",
       "      <td>0.092119</td>\n",
       "      <td>0.140600</td>\n",
       "      <td>0.002546</td>\n",
       "      <td>1701.890924</td>\n",
       "      <td>35678.130616</td>\n",
       "      <td>1979.387612</td>\n",
       "      <td>36670.725886</td>\n",
       "      <td>...</td>\n",
       "      <td>38.456211</td>\n",
       "      <td>-3.637886</td>\n",
       "      <td>24.530296</td>\n",
       "      <td>-0.105148</td>\n",
       "      <td>26.716150</td>\n",
       "      <td>-2.016985</td>\n",
       "      <td>23.150423</td>\n",
       "      <td>0.210787</td>\n",
       "      <td>42.512966</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>blues.00000.8.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.347877</td>\n",
       "      <td>0.094209</td>\n",
       "      <td>0.133130</td>\n",
       "      <td>0.002538</td>\n",
       "      <td>1746.473502</td>\n",
       "      <td>138073.931244</td>\n",
       "      <td>1887.619723</td>\n",
       "      <td>117069.920049</td>\n",
       "      <td>...</td>\n",
       "      <td>44.311455</td>\n",
       "      <td>-4.370029</td>\n",
       "      <td>29.873167</td>\n",
       "      <td>2.114592</td>\n",
       "      <td>33.843155</td>\n",
       "      <td>-2.264663</td>\n",
       "      <td>80.812393</td>\n",
       "      <td>3.758598</td>\n",
       "      <td>97.618835</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>blues.00000.9.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.358061</td>\n",
       "      <td>0.082957</td>\n",
       "      <td>0.115312</td>\n",
       "      <td>0.001846</td>\n",
       "      <td>1763.948942</td>\n",
       "      <td>61493.423121</td>\n",
       "      <td>1874.195710</td>\n",
       "      <td>51944.921435</td>\n",
       "      <td>...</td>\n",
       "      <td>43.967834</td>\n",
       "      <td>-3.448304</td>\n",
       "      <td>48.671944</td>\n",
       "      <td>0.099792</td>\n",
       "      <td>41.839546</td>\n",
       "      <td>-7.677177</td>\n",
       "      <td>96.253654</td>\n",
       "      <td>0.791776</td>\n",
       "      <td>40.416420</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>blues.00001.0.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.402401</td>\n",
       "      <td>0.090340</td>\n",
       "      <td>0.093024</td>\n",
       "      <td>0.003876</td>\n",
       "      <td>1279.182242</td>\n",
       "      <td>406513.816744</td>\n",
       "      <td>1921.306192</td>\n",
       "      <td>196573.441648</td>\n",
       "      <td>...</td>\n",
       "      <td>39.640743</td>\n",
       "      <td>1.263416</td>\n",
       "      <td>22.989195</td>\n",
       "      <td>0.136411</td>\n",
       "      <td>19.644432</td>\n",
       "      <td>-0.883823</td>\n",
       "      <td>20.996868</td>\n",
       "      <td>0.375303</td>\n",
       "      <td>21.973501</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>blues.00001.1.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.345507</td>\n",
       "      <td>0.091037</td>\n",
       "      <td>0.094656</td>\n",
       "      <td>0.001495</td>\n",
       "      <td>1513.763969</td>\n",
       "      <td>214768.804443</td>\n",
       "      <td>2091.432928</td>\n",
       "      <td>121877.017611</td>\n",
       "      <td>...</td>\n",
       "      <td>58.940536</td>\n",
       "      <td>-4.857230</td>\n",
       "      <td>56.075413</td>\n",
       "      <td>-2.132336</td>\n",
       "      <td>28.025103</td>\n",
       "      <td>-0.170657</td>\n",
       "      <td>40.780113</td>\n",
       "      <td>-1.455821</td>\n",
       "      <td>34.120010</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>blues.00001.2.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.338119</td>\n",
       "      <td>0.083682</td>\n",
       "      <td>0.097776</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>1308.869728</td>\n",
       "      <td>154209.122448</td>\n",
       "      <td>1508.008806</td>\n",
       "      <td>111306.051891</td>\n",
       "      <td>...</td>\n",
       "      <td>63.200188</td>\n",
       "      <td>4.592705</td>\n",
       "      <td>79.577530</td>\n",
       "      <td>4.732941</td>\n",
       "      <td>70.588402</td>\n",
       "      <td>-1.336461</td>\n",
       "      <td>105.036049</td>\n",
       "      <td>2.067705</td>\n",
       "      <td>96.283142</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>blues.00001.3.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.330751</td>\n",
       "      <td>0.093936</td>\n",
       "      <td>0.085365</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>1479.530348</td>\n",
       "      <td>646868.373151</td>\n",
       "      <td>1906.454247</td>\n",
       "      <td>357705.425072</td>\n",
       "      <td>...</td>\n",
       "      <td>98.227165</td>\n",
       "      <td>-0.936432</td>\n",
       "      <td>45.998901</td>\n",
       "      <td>-1.938492</td>\n",
       "      <td>34.591671</td>\n",
       "      <td>-4.223902</td>\n",
       "      <td>27.492662</td>\n",
       "      <td>-2.418841</td>\n",
       "      <td>37.799568</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>blues.00001.4.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.348027</td>\n",
       "      <td>0.100096</td>\n",
       "      <td>0.088437</td>\n",
       "      <td>0.002204</td>\n",
       "      <td>1729.640428</td>\n",
       "      <td>568827.519152</td>\n",
       "      <td>2150.738315</td>\n",
       "      <td>153824.455302</td>\n",
       "      <td>...</td>\n",
       "      <td>42.760681</td>\n",
       "      <td>0.101109</td>\n",
       "      <td>62.984756</td>\n",
       "      <td>-0.026374</td>\n",
       "      <td>71.700882</td>\n",
       "      <td>-3.079430</td>\n",
       "      <td>50.402306</td>\n",
       "      <td>2.897624</td>\n",
       "      <td>31.121618</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>blues.00001.5.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.329435</td>\n",
       "      <td>0.104149</td>\n",
       "      <td>0.093201</td>\n",
       "      <td>0.001929</td>\n",
       "      <td>1492.089309</td>\n",
       "      <td>228951.553214</td>\n",
       "      <td>2189.638968</td>\n",
       "      <td>151468.948965</td>\n",
       "      <td>...</td>\n",
       "      <td>37.970470</td>\n",
       "      <td>-1.117381</td>\n",
       "      <td>64.139557</td>\n",
       "      <td>-0.196972</td>\n",
       "      <td>45.080425</td>\n",
       "      <td>-0.722148</td>\n",
       "      <td>28.655661</td>\n",
       "      <td>2.487024</td>\n",
       "      <td>49.899857</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>blues.00001.6.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.343666</td>\n",
       "      <td>0.096098</td>\n",
       "      <td>0.089783</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>1571.420445</td>\n",
       "      <td>257630.094708</td>\n",
       "      <td>2183.513315</td>\n",
       "      <td>158620.665353</td>\n",
       "      <td>...</td>\n",
       "      <td>31.882519</td>\n",
       "      <td>-0.572581</td>\n",
       "      <td>44.982315</td>\n",
       "      <td>-0.877120</td>\n",
       "      <td>31.649658</td>\n",
       "      <td>-3.196097</td>\n",
       "      <td>60.434364</td>\n",
       "      <td>1.792135</td>\n",
       "      <td>47.496124</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>blues.00001.7.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.327732</td>\n",
       "      <td>0.099648</td>\n",
       "      <td>0.110633</td>\n",
       "      <td>0.003306</td>\n",
       "      <td>1877.638297</td>\n",
       "      <td>510311.370648</td>\n",
       "      <td>2326.810264</td>\n",
       "      <td>135085.794816</td>\n",
       "      <td>...</td>\n",
       "      <td>50.891602</td>\n",
       "      <td>-4.773520</td>\n",
       "      <td>40.693054</td>\n",
       "      <td>1.114042</td>\n",
       "      <td>45.685413</td>\n",
       "      <td>-1.074144</td>\n",
       "      <td>63.274281</td>\n",
       "      <td>-0.478155</td>\n",
       "      <td>47.172031</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>blues.00001.8.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.329428</td>\n",
       "      <td>0.090254</td>\n",
       "      <td>0.111013</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>1665.600276</td>\n",
       "      <td>239973.571541</td>\n",
       "      <td>2168.031814</td>\n",
       "      <td>107476.137097</td>\n",
       "      <td>...</td>\n",
       "      <td>45.785587</td>\n",
       "      <td>-6.817122</td>\n",
       "      <td>43.133862</td>\n",
       "      <td>6.550156</td>\n",
       "      <td>45.450241</td>\n",
       "      <td>6.003541</td>\n",
       "      <td>33.629559</td>\n",
       "      <td>2.775560</td>\n",
       "      <td>59.863949</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>blues.00001.9.wav</td>\n",
       "      <td>66149</td>\n",
       "      <td>0.292987</td>\n",
       "      <td>0.082145</td>\n",
       "      <td>0.094924</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>1373.437553</td>\n",
       "      <td>210606.446462</td>\n",
       "      <td>1929.319087</td>\n",
       "      <td>165808.594240</td>\n",
       "      <td>...</td>\n",
       "      <td>40.023655</td>\n",
       "      <td>-2.538458</td>\n",
       "      <td>86.850639</td>\n",
       "      <td>3.124925</td>\n",
       "      <td>32.567192</td>\n",
       "      <td>-0.463293</td>\n",
       "      <td>20.064491</td>\n",
       "      <td>2.491399</td>\n",
       "      <td>18.163389</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             filename  length  chroma_stft_mean  chroma_stft_var  rms_mean  \\\n",
       "0   blues.00000.0.wav   66149          0.335406         0.091048  0.130405   \n",
       "1   blues.00000.1.wav   66149          0.343065         0.086147  0.112699   \n",
       "2   blues.00000.2.wav   66149          0.346815         0.092243  0.132003   \n",
       "3   blues.00000.3.wav   66149          0.363639         0.086856  0.132565   \n",
       "4   blues.00000.4.wav   66149          0.335579         0.088129  0.143289   \n",
       "5   blues.00000.5.wav   66149          0.376670         0.089702  0.132618   \n",
       "6   blues.00000.6.wav   66149          0.379909         0.088827  0.130335   \n",
       "7   blues.00000.7.wav   66149          0.331880         0.092119  0.140600   \n",
       "8   blues.00000.8.wav   66149          0.347877         0.094209  0.133130   \n",
       "9   blues.00000.9.wav   66149          0.358061         0.082957  0.115312   \n",
       "10  blues.00001.0.wav   66149          0.402401         0.090340  0.093024   \n",
       "11  blues.00001.1.wav   66149          0.345507         0.091037  0.094656   \n",
       "12  blues.00001.2.wav   66149          0.338119         0.083682  0.097776   \n",
       "13  blues.00001.3.wav   66149          0.330751         0.093936  0.085365   \n",
       "14  blues.00001.4.wav   66149          0.348027         0.100096  0.088437   \n",
       "15  blues.00001.5.wav   66149          0.329435         0.104149  0.093201   \n",
       "16  blues.00001.6.wav   66149          0.343666         0.096098  0.089783   \n",
       "17  blues.00001.7.wav   66149          0.327732         0.099648  0.110633   \n",
       "18  blues.00001.8.wav   66149          0.329428         0.090254  0.111013   \n",
       "19  blues.00001.9.wav   66149          0.292987         0.082145  0.094924   \n",
       "\n",
       "     rms_var  spectral_centroid_mean  spectral_centroid_var  \\\n",
       "0   0.003521             1773.065032          167541.630869   \n",
       "1   0.001450             1816.693777           90525.690866   \n",
       "2   0.004620             1788.539719          111407.437613   \n",
       "3   0.002448             1655.289045          111952.284517   \n",
       "4   0.001701             1630.656199           79667.267654   \n",
       "5   0.003583             1994.915219          211700.619569   \n",
       "6   0.003166             1962.150096          177443.070045   \n",
       "7   0.002546             1701.890924           35678.130616   \n",
       "8   0.002538             1746.473502          138073.931244   \n",
       "9   0.001846             1763.948942           61493.423121   \n",
       "10  0.003876             1279.182242          406513.816744   \n",
       "11  0.001495             1513.763969          214768.804443   \n",
       "12  0.001386             1308.869728          154209.122448   \n",
       "13  0.002641             1479.530348          646868.373151   \n",
       "14  0.002204             1729.640428          568827.519152   \n",
       "15  0.001929             1492.089309          228951.553214   \n",
       "16  0.002173             1571.420445          257630.094708   \n",
       "17  0.003306             1877.638297          510311.370648   \n",
       "18  0.001072             1665.600276          239973.571541   \n",
       "19  0.003031             1373.437553          210606.446462   \n",
       "\n",
       "    spectral_bandwidth_mean  spectral_bandwidth_var  ...  mfcc16_var  \\\n",
       "0               1972.744388           117335.771563  ...   39.687145   \n",
       "1               2010.051501            65671.875673  ...   64.748276   \n",
       "2               2084.565132            75124.921716  ...   67.336563   \n",
       "3               1960.039988            82913.639269  ...   47.739452   \n",
       "4               1948.503884            60204.020268  ...   30.336359   \n",
       "5               2152.767854            74263.873102  ...   31.448069   \n",
       "6               2146.503479            98020.541422  ...   33.954071   \n",
       "7               1979.387612            36670.725886  ...   38.456211   \n",
       "8               1887.619723           117069.920049  ...   44.311455   \n",
       "9               1874.195710            51944.921435  ...   43.967834   \n",
       "10              1921.306192           196573.441648  ...   39.640743   \n",
       "11              2091.432928           121877.017611  ...   58.940536   \n",
       "12              1508.008806           111306.051891  ...   63.200188   \n",
       "13              1906.454247           357705.425072  ...   98.227165   \n",
       "14              2150.738315           153824.455302  ...   42.760681   \n",
       "15              2189.638968           151468.948965  ...   37.970470   \n",
       "16              2183.513315           158620.665353  ...   31.882519   \n",
       "17              2326.810264           135085.794816  ...   50.891602   \n",
       "18              2168.031814           107476.137097  ...   45.785587   \n",
       "19              1929.319087           165808.594240  ...   40.023655   \n",
       "\n",
       "    mfcc17_mean  mfcc17_var  mfcc18_mean  mfcc18_var  mfcc19_mean  mfcc19_var  \\\n",
       "0     -3.241280   36.488243     0.722209   38.099152    -5.050335   33.618073   \n",
       "1     -6.055294   40.677654     0.159015   51.264091    -2.837699   97.030830   \n",
       "2     -1.768610   28.348579     2.378768   45.717648    -1.938424   53.050835   \n",
       "3     -3.841155   28.337118     1.218588   34.770935    -3.580352   50.836224   \n",
       "4      0.664582   45.880913     1.689446   51.363583    -3.392489   26.738789   \n",
       "5     -3.448373   34.284130    -0.416165   40.791092    -3.649625   32.457901   \n",
       "6     -2.068194   25.623655     1.428141   47.957699    -3.267124   39.382240   \n",
       "7     -3.637886   24.530296    -0.105148   26.716150    -2.016985   23.150423   \n",
       "8     -4.370029   29.873167     2.114592   33.843155    -2.264663   80.812393   \n",
       "9     -3.448304   48.671944     0.099792   41.839546    -7.677177   96.253654   \n",
       "10     1.263416   22.989195     0.136411   19.644432    -0.883823   20.996868   \n",
       "11    -4.857230   56.075413    -2.132336   28.025103    -0.170657   40.780113   \n",
       "12     4.592705   79.577530     4.732941   70.588402    -1.336461  105.036049   \n",
       "13    -0.936432   45.998901    -1.938492   34.591671    -4.223902   27.492662   \n",
       "14     0.101109   62.984756    -0.026374   71.700882    -3.079430   50.402306   \n",
       "15    -1.117381   64.139557    -0.196972   45.080425    -0.722148   28.655661   \n",
       "16    -0.572581   44.982315    -0.877120   31.649658    -3.196097   60.434364   \n",
       "17    -4.773520   40.693054     1.114042   45.685413    -1.074144   63.274281   \n",
       "18    -6.817122   43.133862     6.550156   45.450241     6.003541   33.629559   \n",
       "19    -2.538458   86.850639     3.124925   32.567192    -0.463293   20.064491   \n",
       "\n",
       "    mfcc20_mean  mfcc20_var  label  \n",
       "0     -0.243027   43.771767  blues  \n",
       "1      5.784063   59.943081  blues  \n",
       "2      2.517375   33.105122  blues  \n",
       "3      3.630866   32.023678  blues  \n",
       "4      0.536961   29.146694  blues  \n",
       "5      3.025218   28.892687  blues  \n",
       "6      3.276939   25.999132  blues  \n",
       "7      0.210787   42.512966  blues  \n",
       "8      3.758598   97.618835  blues  \n",
       "9      0.791776   40.416420  blues  \n",
       "10     0.375303   21.973501  blues  \n",
       "11    -1.455821   34.120010  blues  \n",
       "12     2.067705   96.283142  blues  \n",
       "13    -2.418841   37.799568  blues  \n",
       "14     2.897624   31.121618  blues  \n",
       "15     2.487024   49.899857  blues  \n",
       "16     1.792135   47.496124  blues  \n",
       "17    -0.478155   47.172031  blues  \n",
       "18     2.775560   59.863949  blues  \n",
       "19     2.491399   18.163389  blues  \n",
       "\n",
       "[20 rows x 60 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the dataset\n",
    "data_3s = pd.read_csv('../dataset/features_3_sec.csv') # Dataset for the 3 second featureset\n",
    "data_30s = pd.read_csv('../dataset/features_30_sec.csv') # Dataset for the 30 second featureset\n",
    "\n",
    "data = data_3s # Just change this instead of changing every single variable\n",
    "\n",
    "first_20_rows = data.head(20)\n",
    "first_20_rows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rid the dataset of useless rows\n",
    "data = data.drop('filename', axis=1)\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop('label', axis=1)\n",
    "y = data['label']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features (maybe we need this or maybe not, TODO maybe test the results without this)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Train a machine learning model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize and train the classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Keras Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 1.2717 - accuracy: 0.5648 - val_loss: 0.9145 - val_accuracy: 0.6917\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.7934 - accuracy: 0.7326 - val_loss: 0.7470 - val_accuracy: 0.7513\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 0s 992us/step - loss: 0.6435 - accuracy: 0.7869 - val_loss: 0.6597 - val_accuracy: 0.7718\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.5425 - accuracy: 0.8211 - val_loss: 0.6188 - val_accuracy: 0.7868\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 0s 996us/step - loss: 0.4773 - accuracy: 0.8410 - val_loss: 0.5695 - val_accuracy: 0.8073\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.4209 - accuracy: 0.8607 - val_loss: 0.5246 - val_accuracy: 0.8303\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3704 - accuracy: 0.8806 - val_loss: 0.4941 - val_accuracy: 0.8323\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3238 - accuracy: 0.8955 - val_loss: 0.4722 - val_accuracy: 0.8413\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.2851 - accuracy: 0.9112 - val_loss: 0.4804 - val_accuracy: 0.8453\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 0s 970us/step - loss: 0.2583 - accuracy: 0.9165 - val_loss: 0.4519 - val_accuracy: 0.8504\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.2334 - accuracy: 0.9254 - val_loss: 0.4380 - val_accuracy: 0.8594\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.2004 - accuracy: 0.9409 - val_loss: 0.4385 - val_accuracy: 0.8488\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1805 - accuracy: 0.9459 - val_loss: 0.4258 - val_accuracy: 0.8634\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 0s 960us/step - loss: 0.1596 - accuracy: 0.9551 - val_loss: 0.4400 - val_accuracy: 0.8564\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 0s 956us/step - loss: 0.1435 - accuracy: 0.9597 - val_loss: 0.4295 - val_accuracy: 0.8704\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 0s 972us/step - loss: 0.1290 - accuracy: 0.9647 - val_loss: 0.4242 - val_accuracy: 0.8739\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1186 - accuracy: 0.9668 - val_loss: 0.4296 - val_accuracy: 0.8724\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1022 - accuracy: 0.9732 - val_loss: 0.4085 - val_accuracy: 0.8804\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 0s 971us/step - loss: 0.0887 - accuracy: 0.9787 - val_loss: 0.4179 - val_accuracy: 0.8849\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 0s 980us/step - loss: 0.0821 - accuracy: 0.9804 - val_loss: 0.4398 - val_accuracy: 0.8794\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 0s 972us/step - loss: 0.0767 - accuracy: 0.9799 - val_loss: 0.4447 - val_accuracy: 0.8874\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 0s 968us/step - loss: 0.0693 - accuracy: 0.9834 - val_loss: 0.4384 - val_accuracy: 0.8849\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 0s 992us/step - loss: 0.0588 - accuracy: 0.9864 - val_loss: 0.4628 - val_accuracy: 0.8759\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 0s 970us/step - loss: 0.0558 - accuracy: 0.9867 - val_loss: 0.4603 - val_accuracy: 0.8834\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 0s 984us/step - loss: 0.0517 - accuracy: 0.9880 - val_loss: 0.4665 - val_accuracy: 0.8869\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0503 - accuracy: 0.9884 - val_loss: 0.5013 - val_accuracy: 0.8724\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 0s 980us/step - loss: 0.0470 - accuracy: 0.9887 - val_loss: 0.4942 - val_accuracy: 0.8749\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0423 - accuracy: 0.9905 - val_loss: 0.4799 - val_accuracy: 0.8874\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 0s 974us/step - loss: 0.0337 - accuracy: 0.9930 - val_loss: 0.4791 - val_accuracy: 0.8864\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0359 - accuracy: 0.9925 - val_loss: 0.5192 - val_accuracy: 0.8864\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0295 - accuracy: 0.9939 - val_loss: 0.5208 - val_accuracy: 0.8839\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 0s 988us/step - loss: 0.0243 - accuracy: 0.9955 - val_loss: 0.5067 - val_accuracy: 0.8854\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 0s 990us/step - loss: 0.0200 - accuracy: 0.9965 - val_loss: 0.5160 - val_accuracy: 0.8899\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 0s 964us/step - loss: 0.0398 - accuracy: 0.9891 - val_loss: 0.5498 - val_accuracy: 0.8734\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 0s 976us/step - loss: 0.0335 - accuracy: 0.9919 - val_loss: 0.5430 - val_accuracy: 0.8799\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 0s 988us/step - loss: 0.0208 - accuracy: 0.9960 - val_loss: 0.4869 - val_accuracy: 0.8909\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 0s 964us/step - loss: 0.0121 - accuracy: 0.9980 - val_loss: 0.5142 - val_accuracy: 0.8889\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0148 - accuracy: 0.9966 - val_loss: 0.5313 - val_accuracy: 0.8909\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 0s 992us/step - loss: 0.0133 - accuracy: 0.9977 - val_loss: 0.6257 - val_accuracy: 0.8779\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 0s 960us/step - loss: 0.0204 - accuracy: 0.9957 - val_loss: 0.5949 - val_accuracy: 0.8804\n",
      "Epoch 41/100\n",
      "250/250 [==============================] - 0s 973us/step - loss: 0.0640 - accuracy: 0.9810 - val_loss: 0.6078 - val_accuracy: 0.8769\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 0s 968us/step - loss: 0.0369 - accuracy: 0.9880 - val_loss: 0.6084 - val_accuracy: 0.8799\n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 0s 967us/step - loss: 0.0144 - accuracy: 0.9972 - val_loss: 0.5401 - val_accuracy: 0.8954\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 0s 944us/step - loss: 0.0081 - accuracy: 0.9990 - val_loss: 0.5370 - val_accuracy: 0.9004\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 0s 964us/step - loss: 0.0076 - accuracy: 0.9989 - val_loss: 0.5324 - val_accuracy: 0.9039\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 0s 984us/step - loss: 0.0070 - accuracy: 0.9987 - val_loss: 0.5433 - val_accuracy: 0.8974\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 0s 975us/step - loss: 0.0070 - accuracy: 0.9990 - val_loss: 0.5515 - val_accuracy: 0.8994\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 0s 961us/step - loss: 0.0069 - accuracy: 0.9989 - val_loss: 0.5593 - val_accuracy: 0.8934\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 0s 974us/step - loss: 0.0172 - accuracy: 0.9949 - val_loss: 0.5944 - val_accuracy: 0.8954\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 0s 952us/step - loss: 0.0746 - accuracy: 0.9766 - val_loss: 0.6946 - val_accuracy: 0.8789\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 0s 972us/step - loss: 0.0305 - accuracy: 0.9902 - val_loss: 0.5941 - val_accuracy: 0.8984\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 0s 960us/step - loss: 0.0199 - accuracy: 0.9946 - val_loss: 0.5799 - val_accuracy: 0.8944\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 0s 940us/step - loss: 0.0112 - accuracy: 0.9980 - val_loss: 0.6301 - val_accuracy: 0.8864\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 0s 956us/step - loss: 0.0119 - accuracy: 0.9965 - val_loss: 0.5935 - val_accuracy: 0.8994\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 0s 948us/step - loss: 0.0098 - accuracy: 0.9976 - val_loss: 0.5940 - val_accuracy: 0.9009\n",
      "Epoch 56/100\n",
      "250/250 [==============================] - 0s 960us/step - loss: 0.0064 - accuracy: 0.9989 - val_loss: 0.5974 - val_accuracy: 0.8984\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 0s 968us/step - loss: 0.0051 - accuracy: 0.9990 - val_loss: 0.5838 - val_accuracy: 0.9004\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 0s 964us/step - loss: 0.0099 - accuracy: 0.9970 - val_loss: 0.6072 - val_accuracy: 0.9049\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 0s 964us/step - loss: 0.0082 - accuracy: 0.9979 - val_loss: 0.6278 - val_accuracy: 0.8974\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 0s 976us/step - loss: 0.0623 - accuracy: 0.9845 - val_loss: 0.7933 - val_accuracy: 0.8674\n",
      "Epoch 61/100\n",
      "250/250 [==============================] - 0s 956us/step - loss: 0.0580 - accuracy: 0.9811 - val_loss: 0.6432 - val_accuracy: 0.8889\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 0s 964us/step - loss: 0.0113 - accuracy: 0.9979 - val_loss: 0.5905 - val_accuracy: 0.8984\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0053 - accuracy: 0.9989 - val_loss: 0.5877 - val_accuracy: 0.9034\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 0s 972us/step - loss: 0.0037 - accuracy: 0.9994 - val_loss: 0.5875 - val_accuracy: 0.9014\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 0.5882 - val_accuracy: 0.9029\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 0s 960us/step - loss: 0.0045 - accuracy: 0.9986 - val_loss: 0.5929 - val_accuracy: 0.9064\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 0s 964us/step - loss: 0.0041 - accuracy: 0.9987 - val_loss: 0.5990 - val_accuracy: 0.9009\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - 0s 956us/step - loss: 0.0070 - accuracy: 0.9984 - val_loss: 0.6171 - val_accuracy: 0.9024\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 0s 948us/step - loss: 0.0054 - accuracy: 0.9987 - val_loss: 0.6125 - val_accuracy: 0.9059\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - 0s 960us/step - loss: 0.0135 - accuracy: 0.9949 - val_loss: 0.6766 - val_accuracy: 0.8929\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 0s 960us/step - loss: 0.0861 - accuracy: 0.9745 - val_loss: 0.7625 - val_accuracy: 0.8829\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 0s 936us/step - loss: 0.0465 - accuracy: 0.9852 - val_loss: 0.6789 - val_accuracy: 0.8909\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 0s 959us/step - loss: 0.0165 - accuracy: 0.9950 - val_loss: 0.6789 - val_accuracy: 0.8939\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 0s 962us/step - loss: 0.0061 - accuracy: 0.9984 - val_loss: 0.6601 - val_accuracy: 0.9019\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0048 - accuracy: 0.9989 - val_loss: 0.6491 - val_accuracy: 0.9024\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 0s 964us/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.6513 - val_accuracy: 0.9004\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 0.6440 - val_accuracy: 0.9009\n",
      "Epoch 78/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.6503 - val_accuracy: 0.9039\n",
      "Epoch 79/100\n",
      "250/250 [==============================] - 0s 961us/step - loss: 0.0045 - accuracy: 0.9991 - val_loss: 0.6476 - val_accuracy: 0.9004\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 0s 958us/step - loss: 0.0063 - accuracy: 0.9984 - val_loss: 0.6525 - val_accuracy: 0.8969\n",
      "Epoch 81/100\n",
      "250/250 [==============================] - 0s 974us/step - loss: 0.0062 - accuracy: 0.9981 - val_loss: 0.6502 - val_accuracy: 0.8984\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 0s 956us/step - loss: 0.0076 - accuracy: 0.9981 - val_loss: 0.6954 - val_accuracy: 0.8949\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 0s 996us/step - loss: 0.0064 - accuracy: 0.9984 - val_loss: 0.6970 - val_accuracy: 0.8969\n",
      "Epoch 84/100\n",
      "250/250 [==============================] - 0s 936us/step - loss: 0.1157 - accuracy: 0.9703 - val_loss: 0.7677 - val_accuracy: 0.8779\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 0s 964us/step - loss: 0.0341 - accuracy: 0.9892 - val_loss: 0.6401 - val_accuracy: 0.8954\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 0s 969us/step - loss: 0.0100 - accuracy: 0.9974 - val_loss: 0.6451 - val_accuracy: 0.8944\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 0s 959us/step - loss: 0.0046 - accuracy: 0.9992 - val_loss: 0.6339 - val_accuracy: 0.8984\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 0s 984us/step - loss: 0.0035 - accuracy: 0.9991 - val_loss: 0.6273 - val_accuracy: 0.9014\n",
      "Epoch 89/100\n",
      "250/250 [==============================] - 0s 996us/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 0.6295 - val_accuracy: 0.9009\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 0s 968us/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.6322 - val_accuracy: 0.9039\n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0028 - accuracy: 0.9992 - val_loss: 0.6401 - val_accuracy: 0.9034\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 0s 980us/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.6352 - val_accuracy: 0.9034\n",
      "Epoch 93/100\n",
      "250/250 [==============================] - 0s 991us/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.6522 - val_accuracy: 0.9024\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 0s 980us/step - loss: 0.0053 - accuracy: 0.9982 - val_loss: 0.6375 - val_accuracy: 0.9054\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0048 - accuracy: 0.9984 - val_loss: 0.6602 - val_accuracy: 0.9019\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0223 - accuracy: 0.9931 - val_loss: 0.8204 - val_accuracy: 0.8744\n",
      "Epoch 97/100\n",
      "250/250 [==============================] - 0s 980us/step - loss: 0.1191 - accuracy: 0.9660 - val_loss: 0.7878 - val_accuracy: 0.8749\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 0s 972us/step - loss: 0.0159 - accuracy: 0.9951 - val_loss: 0.6577 - val_accuracy: 0.8979\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 0s 965us/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.6616 - val_accuracy: 0.9024\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 0s 956us/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.6445 - val_accuracy: 0.9054\n"
     ]
    }
   ],
   "source": [
    "# Encode genre names to integers\n",
    "encoder = LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n",
    "y_test_encoded = encoder.transform(y_test)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_onehot = to_categorical(y_train_encoded)\n",
    "y_test_onehot = to_categorical(y_test_encoded)\n",
    "\n",
    "# Creating the Keras Neural Network\n",
    "model = Sequential()\n",
    "# Adding Layers (like an onion)\n",
    "# Using Rectified Linear Unit (ReLU)\n",
    "# TODO test using Exponential Linear Units (ELUs) or add another variable to compare it to or something\n",
    "model.add(Dense(128, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax')) # Softmax for output in probabilities\n",
    "\n",
    "# Finialising the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train_onehot, validation_data=(X_test, y_test_onehot), epochs=100, batch_size=32, verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually mapping the old labels with their integer variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blues': 0, 'classical': 1, 'country': 2, 'disco': 3, 'hiphop': 4, 'jazz': 5, 'metal': 6, 'pop': 7, 'reggae': 8, 'rock': 9}\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n",
    "\n",
    "# Print the mapping of genre names to integers\n",
    "genre_mapping = dict(zip(encoder.classes_, range(len(encoder.classes_))))\n",
    "print(genre_mapping)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Evaluating the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Classifier Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       blues       0.87      0.87      0.87       208\n",
      "   classical       0.94      0.98      0.96       203\n",
      "     country       0.80      0.84      0.82       186\n",
      "       disco       0.85      0.83      0.84       199\n",
      "      hiphop       0.92      0.89      0.90       218\n",
      "        jazz       0.86      0.93      0.89       192\n",
      "       metal       0.87      0.95      0.91       204\n",
      "         pop       0.92      0.95      0.93       180\n",
      "      reggae       0.92      0.88      0.90       211\n",
      "        rock       0.89      0.72      0.79       197\n",
      "\n",
      "    accuracy                           0.88      1998\n",
      "   macro avg       0.88      0.88      0.88      1998\n",
      "weighted avg       0.88      0.88      0.88      1998\n",
      "\n",
      "[[180   1   8   4   2   6   3   0   3   1]\n",
      " [  0 199   0   0   0   4   0   0   0   0]\n",
      " [ 13   0 156   4   0   8   2   2   1   0]\n",
      " [  1   1   3 166   8   2   3   2   6   7]\n",
      " [  4   0   4   2 193   1   8   4   1   1]\n",
      " [  2   9   2   1   0 178   0   0   0   0]\n",
      " [  0   0   0   0   1   0 194   0   2   7]\n",
      " [  0   0   3   1   1   2   0 171   0   2]\n",
      " [  2   1   6   6   4   1   0   5 186   0]\n",
      " [  5   1  13  12   1   6  13   2   3 141]]\n",
      "63/63 [==============================] - 0s 565us/step\n",
      "\n",
      "Keras Neural Network Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       blues       0.93      0.88      0.91       208\n",
      "   classical       0.93      0.95      0.94       203\n",
      "     country       0.86      0.90      0.88       186\n",
      "       disco       0.90      0.81      0.85       199\n",
      "      hiphop       0.93      0.92      0.92       218\n",
      "        jazz       0.90      0.93      0.92       192\n",
      "       metal       0.95      0.97      0.96       204\n",
      "         pop       0.91      0.93      0.92       180\n",
      "      reggae       0.90      0.93      0.91       211\n",
      "        rock       0.85      0.84      0.84       197\n",
      "\n",
      "    accuracy                           0.91      1998\n",
      "   macro avg       0.90      0.91      0.90      1998\n",
      "weighted avg       0.91      0.91      0.91      1998\n",
      "\n",
      "[[183   1   7   1   0   7   1   0   3   5]\n",
      " [  2 192   0   0   0   6   0   1   0   2]\n",
      " [  4   1 167   5   0   3   0   3   1   2]\n",
      " [  1   3   4 161   7   0   1   5   5  12]\n",
      " [  1   2   4   1 201   0   0   3   4   2]\n",
      " [  2   6   2   0   0 178   0   0   2   2]\n",
      " [  2   0   2   1   0   0 197   0   0   2]\n",
      " [  0   0   2   1   2   1   0 168   4   2]\n",
      " [  0   1   3   2   5   1   0   1 197   1]\n",
      " [  1   1   3   7   2   1   9   4   4 165]]\n"
     ]
    }
   ],
   "source": [
    "# RandomForest Classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"RandomForest Classifier Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Keras Neural Network\n",
    "# Get the genre names from the encoder\n",
    "genre_names = encoder.classes_\n",
    "\n",
    "y_pred_onehot = model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred_onehot, axis=1)\n",
    "y_pred_genres = encoder.inverse_transform(y_pred_labels)\n",
    "\n",
    "print(\"\\nKeras Neural Network Report:\")\n",
    "print(classification_report(y_test, y_pred_genres, target_names=genre_names))\n",
    "print(confusion_matrix(y_test_encoded, y_pred_labels))\n",
    "\n",
    "# # Keras Neural Network\n",
    "# y_pred_onehot = model.predict(X_test)\n",
    "# y_pred_labels = np.argmax(y_pred_onehot, axis=1)\n",
    "# print(\"\\nKeras Neural Network Report:\")\n",
    "# print(classification_report(y_test_encoded, y_pred_labels))\n",
    "# print(confusion_matrix(y_test_encoded, y_pred_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Comparing the Model Performance\n",
    "\n",
    "[add some waffle here about that or something]\n",
    "[Make sure to change this as we add new models to compare to]\n",
    "\n",
    "##### 5.1 - Accuracy\n",
    "This is the ratio of the number of correct predictions to the total number of predictions. In this case, the RandomForest Classifier has an accuracy of 88%, while the Keras Neural Network has an accuracy of 87%. Both models perform relatively well, with RandomForest having a slightly higher accuracy.\n",
    "\n",
    "#### 5.2 - Precision\n",
    "This is the ratio of true positive predictions to the sum of true positive and false positive predictions. It measures how well the model correctly identifies positive samples. For both models, you can see that the precision values for each genre vary. Overall, RandomForest and Keras perform similarly in terms of precision.\n",
    "\n",
    "#### 5.3 - Recall\n",
    "This is the ratio of true positive predictions to the sum of true positive and false negative predictions. It measures the ability of the model to find all the positive samples. Similar to precision, recall values also vary for each genre, and both models have comparable recall scores.\n",
    "\n",
    "#### 5.4 - F1-score\n",
    "The F1-score is the harmonic mean of precision and recall. It balances both precision and recall, providing a single metric for comparison. For most genres, the F1-scores are close between the RandomForest and Keras models.\n",
    "\n",
    "#### 5.5 - Summary\n",
    "Generally you can see that accross all the numerical metrics the Keras Neural Net performed slightly better\n",
    "\n",
    "You can see however that there are notable differences between the performances in each genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Maybe even add some code too idk figure it out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Make predictions\n",
    "Here we choose the better model to make the predictions and actually do some predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
